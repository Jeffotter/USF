Hi,
My name is Jeff Ott and I am a graduate student as the USF Masters in Data Science program. During this program we've tackle a large amount of topics and projects. I will post what projects I can, but at the request of the univeristy the code will not be readily avaiable unless by specific request. 

**Data translation pipeline** <br>
Description: In this project we made some pyfiles to translate to different data types from command line <br><br>
<img width="509" alt="image" src="https://user-images.githubusercontent.com/60712345/161851380-3bef0ea3-8560-46c0-b48f-fc1c0e26d7c7.png"> <br>
*Libraries Used: sys, untangle, xmltodict, json*




**Search Engine Implementation** <br>
Description: In this project we implimented a search engine with both linear search and hash table search then compared the differences between the two. We then created a local website on flask allow local users to access and use the engine <br><br>
<img width="711" alt="image" src="https://user-images.githubusercontent.com/60712345/161855175-0be40807-edf6-4241-8148-9d1938792bb5.png"><br>
*Libraries Used: Flask, doc2vec, Regex, Codecs, Numpy*


**TFIDF Document Summary** <br>
Description: In this project we processed zipped XML data (44M uncompressed, 9164 files), removed the XML and tokenized the remaining strings. We developed a workflow which would calculat TFIDF (Term Frequency. inverse document Frequency) for each of the documents. 
*Libraries Used: nltk, xml.etree.cElementTree, sklearn.feature_extraction.text, collection, zipfile, string

**Recommendation of Articles** <br>
In this project I was first introduced to word embeddings in the form of word2vec. I converted all the documents in an into embedding lists and found the centriods of each doucment. I then recomended documents based on eucliedain distance. We then use flask, gunicorn, and jninja to build a scaleable website hosted on EC2 on AWS.
*Libraries Used: flask, doc2vec,re,string,numpy, codecs <br>
![image](https://user-images.githubusercontent.com/60712345/163700421-ac099934-f0f7-4739-b00e-57118db89d7f.png)

**Tweet Sentiment Analysis** <br>
In this project I learned how to mine twitter data and perform a sentiment analysis to find the users average sentitment through a search. I then hosted this website on EC2 and let users search for average sentitment on any public twitter handle. This introduced me to website api and classifying sentiment from raw text. 
*Libraries Used: flask, tweetie,colour,numpy, tweepy, vadarSentiment <br>
![image](https://user-images.githubusercontent.com/60712345/163700854-d423e284-d7e2-42c7-8ce4-e2bb6859aff0.png)

Linear Models <br>
Libraries Used:

Naive Bayes<br>
Libraries Used:

Decison Trees<br>
Libraries Used:

Random Forest<br>
Libraries Used:

OO hash table implimentation <br>
Libraries Used:

Clustering <br>
Libraries Used:

Feature Importance <br>
Libraries Used:

Multi Class Logistic Regression Implementation <br>
Libraries Used:

Feature Engineering <br>
Libraries Used:

ML Metric Understanding <br>
Libraries Used:

A/B Testing Netflix Consultation <br>
Libraries Used:




